{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC180 NLP Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying NER with nltk\n",
    "following this tutorial:\n",
    "https://www.youtube.com/watch?v=LFXsG7fueyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            \n",
    "            namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence tagging with LSTM-CRFs\n",
    "from: https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ner_data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\").tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1048565</td>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048566</td>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048567</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048568</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048569</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048570</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048571</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048572</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048573</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048574</td>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = getter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 75\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19296"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"Obama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"B-geo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source activate myenv\n",
    "#conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - //www.github.com/keras-team/keras-contrib.git\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-6009dd8a7b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_contrib'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CRF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-25f358da181c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            recurrent_dropout=0.1))(model)  # variational biLSTM\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# a dense layer as suggested by neuralNer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tags\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CRF layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CRF' is not defined"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words + 1, output_dim=20,\n",
    "                  input_length=max_len, mask_zero=True)(input)  # 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags)  # CRF layer\n",
    "out = crf(model)  # output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition using Bidirectional LSTM-CRF\n",
    "from: https://medium.com/@utkarsh.kumar2407/named-entity-recognition-using-bidirectional-lstm-crf-9f4942746b3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install extra-dependencies\n",
    "#! pip3 -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 5  # Number of passes through entire dataset\n",
    "    MAX_LEN = 75  # Max length of review (in words)\n",
    "    EMBEDDING = 40  # Dimension of word embedding vector\n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 5\n",
    "    MAX_LEN = 75\n",
    "    EMBEDDING = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  47959\n",
      "Number of words in the dataset:  35178\n",
      "Tags: ['I-art', 'I-tim', 'B-eve', 'B-nat', 'B-art', 'I-geo', 'B-per', 'I-gpe', 'I-eve', 'I-per', 'I-nat', 'B-geo', 'B-org', 'O', 'I-org', 'B-tim', 'B-gpe']\n",
      "Number of Labels:  17\n",
      "What the dataset looks like:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)\n",
    "print(\"What the dataset looks like:\")\n",
    "# Show the first 10 rows\n",
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what a sentence looks like:\n",
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    \"\"\"Class to Get the sentence in this format:\n",
    "    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Args:\n",
    "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        \"\"\"Return one sentence\"\"\"\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "print('This is what a sentence looks like:')\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all the sentences\n",
    "sentences = getter.sentences\n",
    "# Plot sentence by lenght\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.title('Token per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Obama is identified by the index: 19297\n",
      "The labels B-geo(which defines Geopraphical Enitities) is identified by the index: 12\n",
      "Raw Sample:  Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Raw Label:  O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "After processing, sample: [11271 34869  5228 12463 29588 19673 17797 21361 33005 27144 33539 22221\n",
      "  7820  2061 22461 27144 17899 34869 19099 30513  1047 22761 29328 30245\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "After processing, labels: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Key:word -> Value:token_index\n",
    "# The first 2 entries are reserved for PAD and UNK\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "print(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\n",
    "print(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Convert each sentence from list of Token to list of word_index\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "# Convert Tag/Label to tag_index\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "from keras.utils import to_categorical\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
    "print('Raw Sample: ', ' '.join([w[0] for w in sentences[0]]))\n",
    "print('Raw Label: ', ' '.join([w[2] for w in sentences[0]]))\n",
    "print('After processing, sample:', X[0])\n",
    "print('After processing, labels:', y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf2crf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3bf50be1978c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf2crf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Model definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf2crf'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tf2crf import CRF\n",
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) with BERT in Spark NLP\n",
    "from: https://towardsdatascience.com/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    py4j-0.10.7                |           py37_0         243 KB\n",
      "    pyspark-2.4.0              |           py37_0       195.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       195.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  py4j               pkgs/main/osx-64::py4j-0.10.7-py37_0\n",
      "  pyspark            pkgs/main/osx-64::pyspark-2.4.0-py37_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pyspark-2.4.0        | 195.5 MB  | ##################################### | 100% \n",
      "py4j-0.10.7          | 243 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - spark-nlp\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    spark-nlp-2.7.1            |           py37_0          31 KB  johnsnowlabs\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          31 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  spark-nlp          johnsnowlabs/noarch::spark-nlp-2.7.1-py37_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "spark-nlp-2.7.1      | 31 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c johnsnowlabs spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.7.1\n",
      "Apache Spark version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "readDataset() missing 2 required positional arguments: 'spark' and 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-3653c6d15163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#training_data = CoNLL().readDataset(spark, './eng.train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#training_data.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mCoNLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: readDataset() missing 2 required positional arguments: 'spark' and 'path'"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "#training_data = CoNLL().readDataset(spark, './eng.train')\n",
    "#training_data.show()\n",
    "CoNLL().readDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-048308d2ae83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./src/main/resources/anc-pos-corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sparknlp/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.johnsnowlabs.nlp.training.POS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputPosCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputDocumentCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputTextCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sparknlp/internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sparknlp/internal.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m     62\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36m_jvm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import POS\n",
    "train_pos = POS().readDataset(spark, \"./src/main/resources/anc-pos-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n",
    "- https://www.youtube.com/watch?v=LFXsG7fueyk\n",
    "- https://www.depends-on-the-definition.com/tags/named-entity-recognition/\n",
    "    - https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/\n",
    "- https://medium.com/@utkarsh.kumar2407/named-entity-recognition-using-bidirectional-lstm-crf-9f4942746b3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
